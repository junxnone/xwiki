---
Title | Hardware Precision
-- | --
Created @ | `2023-07-14T08:35:15Z`
Updated @| `2023-07-14T08:35:15Z`
Labels | ``
Edit @| [here](https://github.com/junxnone/xwiki/issues/281)

---
# Precision
- FP32
- FP16
- BF16
- FP64
- FP80

## FP32
- `float`
- `tf.float32`
- `torch.float32`/`torch.float`


![image](https://github.com/junxnone/xwiki/assets/2216970/4dd6d1c3-7013-4629-9c0a-cad7d4c4fcf0)

## FP16
- `short float`
- `tf.float16`
- `torch.float16` / `torch.half`

![image](https://github.com/junxnone/xwiki/assets/2216970/73897472-5144-4589-8289-ae44755d5ed2)


## BF16
- `tf.bfloat16`
- `torch.bfloat16`

![image](https://github.com/junxnone/xwiki/assets/2216970/0843c1fb-5d83-4820-add3-d9d243834158)

### bf16 vs fp16 vs fp32

![image](https://github.com/junxnone/xwiki/assets/2216970/d637dc8a-c8c9-447d-b289-fe81b555faf7)


## FP64
- `double`
- `tf.float64`
- `torch.float64` / `torch.double`


![image](https://github.com/junxnone/xwiki/assets/2216970/d0e4678d-179e-4512-a5bf-b3ec85001212)


## FP80 
- `long double`

![image](https://github.com/junxnone/xwiki/assets/2216970/e693933c-eb9d-4d67-b669-844ba699ca3e)


